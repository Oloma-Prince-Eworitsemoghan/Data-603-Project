---
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
title: ""
author: ""
date: ""
---

\thispagestyle{empty}


\vspace*{5cm}

\begin{center}
    {\LARGE \textbf{Predicting Boston Housing Prices}}\\[1em]
    {\large Akinyemi Apampao, xxxx}\\[1em]
    {\large David Fakolujo, xxxx}\\[1em]
    {\large Joshua Ogunbo, xxxx}\\[1em]
    {\large Prince Oloma, xxxx}\\[1em]
    {\large Ravin Jayasuriya, xxxx}\\
\end{center}

\newpage

```{r, echo = FALSE}
suppressPackageStartupMessages({
  library(mctest)
  library(ggplot2)
  library(GGally)
  library(car)
  library(olsrr)
  library(lmtest)
  library(knitr)
  })
```

# 1. INTRODUCTION

### 1.1.1 Context

This project falls within the domains of real estate analytics, urban economics, and urban planning. It focuses on understanding the factors that influence housing prices in Boston by analyzing a dataset derived from a census survey conducted in the 1970s. The dataset includes 13 features that may impact the value of homes in different neighborhoods. Our goal is to build regression models to predict the median value of owner-occupied homes (measured in thousands of dollars) and to identify which features have the most significant effect on housing prices.

### 1.1.2 Problem

The problem we aim to address is the difficulty in understanding which features most influence housing prices in Boston. Without clear insights into what drives property values, it becomes challenging for prospective buyers, analysts, or stakeholders to make informed decisions. Our objective is to develop a predictive model that estimates housing prices and helps identify the key factors contributing to those predictions.


### 1.1.3 Challenges

// ADD Data normality issue

\
\


# 1.2 OBJECTIVES

### 1.2.1 Overview

The housing market has been unpredictable in recent years, with prices rising in many areas and making it harder for people, especially young or first-time buyers, to afford a home. In this project, we’re working with housing data from Boston to build a model that can predict house prices. By exploring which features of a home are most strongly linked to its value, we hope to better understand what drives housing prices and help future buyers know what to look for.


### 1.2.2 Goals & Research Questions

The primary goal of this project is to build a predictive model that accurately estimates housing prices in Boston based on various property features. In doing so, we aim to uncover which features most strongly influence a home’s value.

To guide this objective, we explore the following research questions:

- Can we develop a reliable model to predict the median value of homes in Boston?  
- Which features are the most important in influencing housing prices?

\
\
\

# 2. METHODOLOGY

### 2.1 Data
The dataset used in this project consists of housing data collected from the Boston Standard Metropolitan Statistical Area (SMSA) in the 1970s. It contains 506 entries and includes 11 qualitative independent variables, 2 quantitative independent variables, and 1 quantitative dependent variable.

The dataset was originally collected as part of a census report and is considered open data. It is publicly available at:  
[https://lib.stat.cmu.edu/datasets/boston](https://lib.stat.cmu.edu/datasets/boston)

Below is a brief description of each variable:

- **CRIM**: Per capita crime rate by town. Indicates the level of crime in the area.
- **ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft. Reflects residential density.
- **INDUS**: Proportion of non-retail business acres per town. Indicates commercial land usage.
- **CHAS**: Charles River dummy variable (1 if tract bounds river; 0 otherwise). Indicates proximity to the Charles River.
- **NOX**: Nitric oxides concentration (parts per 10 million). Represents industrial pollution.
- **RM**: Average number of rooms per dwelling. Suggests spaciousness.
- **AGE**: Proportion of owner-occupied units built prior to 1940. Reflects the age of buildings in the area.
- **DIS**: Weighted distances to five Boston employment centres. Measures accessibility to work locations.
- **RAD**: Index of accessibility to radial highways. Higher values indicate better road access.
- **TAX**: Full-value property-tax rate per $10,000. Indicates the annual property tax burden.
- **PTRATIO**: Pupil-teacher ratio by town. Lower values suggest better educational facilities.
- **B**: 1000(Bk - 0.63)², where Bk is the proportion of Black residents by town.
- **LSTAT**: Percentage of the population considered lower status.
- **MEDV**: Median value of owner-occupied homes in $1000s. This is the dependent variable we aim to predict.[1]

### 2.2 Approach

In this project, we use a **predictive modeling approach** based on **multiple linear regression** to estimate the median value of homes in Boston. This method is well-suited for our goal of understanding how different housing features influence prices while also producing accurate predictions.

We believe multiple linear regression is an effective choice for this project for several reasons:

- **Interpretability**: The model provides clear and meaningful insights into how each variable affects housing prices, which is valuable for both analysis and decision-making.

- **No Multicollinearity**: VIF results confirmed the absence of multicollinearity, supporting the reliability and stability of the regression coefficients.

- **Well-Structured Data**: The dataset consists of numeric features that align well with the assumptions of linear regression, making it a natural modeling choice.

- **Proven Technique**: Linear regression is a widely accepted method in real estate analytics, with a long history of successful application in similar predictive tasks.

- **Strong Baseline**: This approach serves as a solid baseline for future comparisons with more complex models if needed, offering a balance of performance and simplicity.

### 2.3 Workflows

The workflow for this project follows a typical supervised learning pipeline. Below are the key steps:

1. Read the dataset and view the head

```{r echo = FALSE}
boston_data= read.csv("./BostonHousing.csv")
head(boston_data)
```

2. Modify the categorical variable to ensure the stepwise approach can run
```{r}
boston_data$chas[which(boston_data$chas==0)] = "No"
boston_data$chas[which(boston_data$chas==1)] = "Yes"
```


1. **Testing for Multicollinearity**  
   From the below, multicollinearity was not detected for any of the variables. 



```{r echo = FALSE}
boston_additive = lm(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+b+lstat, data= boston_data)

imcdiag(boston_additive, method = "VIF")
```

```{r echo = FALSE}
vif_table <- data.frame(
  "Variable Name" = c("crim","zn","indus","chas","nox","rm","age","dis","rad","tax","ptratio","b","lstat"),
  "VIF" = c(1.7922,2.2928,3.9916,1.0740,4.3937,1.9337,3.1008,3.9559,7.4845,9.0086,1.7991,1.3485,2.9415),
  "Detection" = c(0,0,0,0,0,0,0,0,0,0,0,0,0)
)
kable(vif_table, caption = "Table 1.0 - TEST FOR MULTICOLINEARITY")
```

b. Additive model
```{r}
summary(boston_additive)
```

Using and individual t-test with the hypothesis: 

$$
\begin{aligned}
H_0&:\beta_1=\beta_2=\beta_p=0\\
H_a&:\mbox{at least one }\beta_i\mbox{ is not zero } (i=1,2,...,p) 
\end{aligned}
$$
The p-value for indus and age are  0.738288 and 0.958229, respectively. Because these p-values are greater than our $\alpha$ of 0.05, we fail to reject the null hypotheses and beta values for these variables are determined to be insignificant.

The model was fit without the insignificant variables

```{r}
reduced_additive_model = lm(formula = medv ~ crim + zn + factor(chas) + nox + rm+ dis + rad + tax + ptratio + b + lstat, data = boston_data)
summary(reduced_additive_model)
```

The reduced model shows that all variables have p-values that are less than 0.05, making them significant. 

We will run a global f-test/anova to check if we should keep the reduced model. 

Hypotheses
$$
\begin{aligned}
H_0&:\beta_i=0\\
H_a&:\beta_i\neq0 (i=1,2,...,p)
\end{aligned}
$$


```{r}
table1 = data.frame((anova(reduced_additive_model,boston_additive)))
print(table1)
```

Since the p_value is 0.9443 which is greater than 0.05, this indicates that we do not have enough evidence against the null hypothesis. Therefore, we can conclude that the reduced model is preferred.

Stepwise selection method was also used to check for the best additive model

```{r}
stepmod=ols_step_both_p(boston_additive,p_enter = 0.05, p_remove = 0.1, details=FALSE)
summary(stepmod$model)
```

The best model from the stepwise selection method above is the same as the reduced model gotten earlier. Hence, the final additive model is: 

$\hat{medv} = 36.341145 - 0.108413crim_{i} +0.045845zn_{i} +2.718716chas_{i} -17.376023nox_{i} + 3.801579rm_{i} -1.492711dis_{i} + 0.299608rad_{i} -0.011778tax_{i} -0.946525ptratio_{i} +0.009291b_{i} - 0.522553lstat_{i}$

where $chas_{i}$ is 1 if the tract bounds Charles River and 0 if otherwise

d. Interaction model

A full two-way interaction model was created with all interaction terms based on the best additive model

```{r}
interaction_model = lm(formula = medv ~ (crim + zn + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat)^2, data = boston_data)
summary(interaction_model)
```

The full interactive model has some interactions with p-values greater than 0.05. These insignificant interactions were dropped to create a reduced interactive model.

```{r}
reduced_interaction_model = lm(formula = medv ~ crim + zn + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas + crim:nox + crim:rm+ crim:dis +crim:rad+ crim:tax+ crim:b +crim:lstat + zn:dis + zn:tax + chas:nox +chas:rm +chas:lstat + nox:rad + nox:tax+ rm:dis + rm:ptratio+ rm:b+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(reduced_interaction_model)
```

The output of the reduced interactive model showed more insignificant interactions, which were further dropped.

```{r}
reduced_interaction_model2 = lm(formula = medv ~ crim + zn + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + nox:tax+ rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(reduced_interaction_model2)
```

The further reduced model then showed nox:tax as insignificant, which was also dropped.

```{r}
reduced_interaction_model3 = lm(formula = medv ~ crim + zn + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(reduced_interaction_model3)
```

Now that all interactions were significant, an f-test was run to compare reduced interactive model and full interaction model

```{r}
table2= data.frame((anova(reduced_interaction_model3,interaction_model)))
print(table2)
```

Based on the summary above, the p-value is $8.242e-12<0.05$, suggesting the null hypothesis should be rejected. Also, the adjusted $R^2_{adj}$ and RSE of the full interaction model are 0.888 and 3.078 respectively, while the adjusted $R^2_{adj}$ and RSE of the reduced interaction model are 0.8627 and 3.408 respectively. 

These suggest the full interaction model should be preferred. However, the full interaction model has a number of insignificant interactions, while the reduced interaction model has only significant interactions. Even though the anova test, adjusted $R^2_{adj}$ and RSE suggest preferring the full interaction model, there isn't a major difference between the adjusted $R^2_{adj}$ and RSE of the two models.

We would choose the reduced model because it retains all significant interactions while eliminating insignificant ones, ensuring better interpretability and avoiding unnecessary complexity without a substantial loss in explanatory power.



# HIGHER ORDER EXPLORATION

To check for possible higher order relationships, we explored all pairwise combinations of continuous variables in scatterplots to see how the response variable looked with respect to each of the continuous additive predictors

```{r}
higher_order_data = data.frame(boston_data$medv, boston_data$crim, boston_data$zn, boston_data$chas, boston_data$nox, boston_data$rm, boston_data$dis, boston_data$rad, boston_data$tax, boston_data$ptratio, boston_data$b, boston_data$lstat)

ggpairs(higher_order_data,lower = list(continuous = wrap("smooth_loess", color = "red"), combo = "facethist", discrete = "facetbar", na = "na"))
```

It looks like the variables that might be worth exploring for possible higher order relationships with medv are crim, zn, nox, rm, dis, rad, tax, lstat. Each of these variables were tested as follows:

# Possible higher order relationship exploration for crim

```{r}
higher_order_crim_2 = lm(formula = medv ~ crim + I(crim^2) + zn + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_crim_2)
```

```{r}
higher_order_crim_3 = lm(formula = medv ~ crim + I(crim^2) + I(crim^3) + zn + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_crim_3)
```

# Possible higher order relationship exploration for zn
```{r}
higher_order_zn_2 = lm(formula = medv ~ crim + zn + I(zn^2) + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_zn_2)
```

# Possible higher order relationship exploration for nox
```{r}
higher_order_nox_2 = lm(formula = medv ~ crim + zn + chas + nox + I(nox^2) + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_nox_2)
```

```{r}
higher_order_nox_3 = lm(formula = medv ~ crim + zn + chas + nox + I(nox^2) + I(nox^3) + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_nox_3)
```

# Possible higher order relationship exploration for rm
```{r}
higher_order_rm_2 = lm(formula = medv ~ crim + zn + chas + nox + rm+ I(rm^2)+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_rm_2)
```

```{r}
higher_order_rm_3 = lm(formula = medv ~ crim + zn + chas + nox + rm+ I(rm^2)+ I(rm^3) + dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_rm_3)
```

# Possible higher order relationship exploration for dis
```{r}
higher_order_dis_2 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + I(dis^2)+ rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_dis_2)
```


```{r}
higher_order_dis_3 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + I(dis^2)+ I(dis^3)+ rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_dis_3)
```


# Possible higher order relationship exploration for rad
```{r}
higher_order_rad_2 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + I(rad^2)+ tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_rad_2)
```

# Possible higher order relationship exploration for tax
```{r}
higher_order_tax_2 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + I(tax^2)+ ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_tax_2)
```

# Possible higher order relationship exploration for lstat
```{r}
higher_order_lstat_2 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_2)
```

```{r}
higher_order_lstat_3 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_3)
```

```{r}
higher_order_lstat_4 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_4)
```

```{r}
higher_order_lstat_5 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_5)
```

```{r}
higher_order_lstat_6 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5)+ I(lstat^6)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_6)
```

Based on the exploration above, the significant higher order relationships were then added to the reduced interactive model

```{r}
higher_order_interaction_model = lm(formula = medv ~ crim + I(crim^2)+ zn + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_interaction_model)
```

The new insignificant variables were then dropped to get a reduced higher order interaction model.

```{r}
higher_order_interaction_model_2 = lm(formula = medv ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_interaction_model_2)
```
The higher order interaction model with significant variables is:

$\hat{medv} = 36.341145 - 0.108413crim_{i} +0.045845zn_{i} +2.718716chas_{i} -17.376023nox_{i} + 3.801579rm_{i} -1.492711dis_{i} + 0.299608rad_{i} -0.011778tax_{i} -0.946525ptratio_{i} +0.009291b_{i} - 0.522553lstat_{i}$

where $chas_{i}$ is 1 if the tract bounds Charles River and 0 if otherwise

# Multiple Regression Assumptions
The following multiple regression assumptions were tested to check if the model is trustworthy

# Linearity Assumption

A scatter plot of the distribution of residuals (errors) vs fitted values (predicted values) was plotted below

```{r}
ggplot(higher_order_interaction_model_2, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)+
ggtitle("Residual plot: Residual vs Fitted values")
```

There appears to be no pattern of the residuals at all, indicating that the model passes the linearity assumption that there is a straight-line (linear) relationship between the predictors and the response.

## Independence Assumption

In the Boston housing dataset, the subjects were not related to time, space, or group, so we can be pretty sure that their measurements are independent.


## Equal Variance Assumption

```{r}
ggplot(higher_order_interaction_model_2, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4")+
ggtitle("Residual plot: Residual vs Fitted values")
```

Another thing we can look at is the scale-location plot between fitted values and standardized residuals to show if residuals are spread equally along the ranges of predictors.


```{r}
ggplot(higher_order_interaction_model_2, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth( colour = "green4")+
ggtitle("Scale-Location plot : Standardized Residual vs Fitted values")
```

Based on the plot, we can see that the scale-location plot is quite horizontal, and there is not any funneling in the residual plot


The Br

```{r}
bptest(higher_order_interaction_model_2)
```

```{r}
higher_order_interaction_model_log = lm(formula = log(medv) ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_interaction_model_log)
```

```{r}
bptest(higher_order_interaction_model_log)
```
```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_log))
```


```{r}
higher_order_interaction_model_log_2 = lm(formula = log(medv) ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ dis:rad + dis:lstat, data = boston_data)
summary(higher_order_interaction_model_log_2)
```

```{r}
bptest(higher_order_interaction_model_log_2)
```

```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_log_2))
```


```{r}
higher_order_interaction_model_log_3 = lm(formula = log(medv) ~ crim + I(crim^2) + chas + nox+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio + dis:lstat, data = boston_data)
summary(higher_order_interaction_model_log_3)
```

```{r}
bptest(higher_order_interaction_model_log_3)
```

```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_log_3))
```



```{r}
library(MASS)

bc=boxcox(higher_order_interaction_model_2,lambda=seq(-1,1))
```


```{r}
#extract best lambda
bestlambda=bc$x[which(bc$y==max(bc$y))]
bestlambda
```


```{r}
higher_order_interaction_model_2_box = lm(formula = (((medv^0.4949495)-1)/0.4949495) ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_interaction_model_2_box)
```

```{r}
bptest(higher_order_interaction_model_2_box)
```


```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_2_box))
```


## Weighted Least Squares Regression Method

```{r}
resid = resid(higher_order_interaction_model_2)
```

```{r}
resid_model = lm(formula = log(resid^2) ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5), data = boston_data)
```

```{r}
fitted_var = exp(fitted(resid_model))
weights = 1 / fitted_var
```


```{r}
higher_order_interaction_model_wls = lm(formula = medv ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ dis:rad + dis:lstat+ b:lstat, data = boston_data, weights = weights)
summary(higher_order_interaction_model_wls)
```


```{r}
bptest(higher_order_interaction_model_wls)
```


```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_wls))
```





```{r}
higher_order_interaction_model_wls_2 = lm(formula = medv ~ crim + I(crim^2) + chas + nox + rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio + dis:lstat+ b:lstat, data = boston_data, weights = weights)
summary(higher_order_interaction_model_wls_2)
```


```{r}
higher_order_interaction_model_wls_3 = lm(formula = medv ~ crim + I(crim^2) + chas + nox + rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + rm:dis + rm:ptratio + dis:lstat+ b:lstat, data = boston_data, weights = weights)
summary(higher_order_interaction_model_wls_3)
```

```{r}
bptest(higher_order_interaction_model_wls_3)
```

```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_wls_3))
```

# CONFIRM LINEARITY ASSUMPTION IS STILL GOOD

```{r}
ggplot(higher_order_interaction_model_wls_3, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4")+
ggtitle("Residual plot: Residual vs Fitted values")
```

```{r}
ggplot(higher_order_interaction_model_wls_3, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)
```
































## NORMALITY

```{r}
ggplot(data=boston_data, aes(residuals(higher_order_interaction_model_2))) +
geom_histogram(breaks = seq(-1,1,by=0.1), col="green3", fill="green4") +
labs(title="Histogram for residuals") +
labs(x="residuals", y="Count")
```


```{r}
ggplot(boston_data, aes(sample=higher_order_interaction_model_2$residuals)) +
stat_qq() + stat_qq_line()
```


```{r}
hist(residuals(higher_order_interaction_model_2))
plot(higher_order_interaction_model_2, which=2)
```

```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_2))
```

## OUTLIERS


```{r}
plot(higher_order_interaction_model_2, 5)
```

Based on this plot everything looks okay
Now I will look at the cooks distance and leverages plots

```{r}
boston_data[cooks.distance(higher_order_interaction_model_2)>0.5,]
```


```{r}
plot(higher_order_interaction_model_2,pch=18,col="red",which=c(4))
```


```{r}
lev=hatvalues(higher_order_interaction_model_2)
p = length(coef(higher_order_interaction_model_2))
n = nrow(boston_data)
outlier2p = lev[lev>(2*p/n)]
outlier3p = lev[lev>(3*p/n)]
print("h_I>2p/n, outliers are")

print(outlier2p)
```



```{r}
print("h_I>3p/n, outliers are")

print(outlier3p)
```

```{r}
plot(rownames(boston_data),lev, main = "Leverage in Boston Housing Dataset", xlab="observation", ylab = "Leverage Value")
abline(h = 2 *p/n, lty = 1)
abline(h = 3 *p/n, lty = 1)
```



It appears we have high leverage points but none of them appear to be particularly influential (no points with a concerning cooks distance). It appears we do not have any outliers that could pose problems.






























stepwise
ols_backwards
ols_forward
Best Subset
HigherOrder
d. R squared & RSE
e. 

g.  

2.4 Contributions

3. MAIN RESULTS OF THE ANALYSIS
3.1 Results
Individual T-test: 

Hypothesis: 


Based off the values indus and age do not seem relevent. 

We will verify utilizing a global anova f-test: 
```{r}


```
Fail to reject null, they are not relevent. 


Comparison Of R^2 and RSE 

Chas is a factor. 


Final Additive Model: 

```{r}
#confint(boston_additive_dropped)
```



4. Conclusion and Discussion
4.1 Approach
4.2 Future Work.



### 2.4 Contributions

In this project, we use a **predictive modeling approach** based on **multiple linear regression** to estimate the median value of homes in Boston. This method is well-suited for our goal of understanding how different housing features influence prices while also producing accurate predictions.

\
\
\

# 3. MAIN RESULT OF THE ANALYSIS


\
\
\

# 4. CONCLUSION AND DISCUSSION

\
\
\

# 5. REFERENCES
[1] - Dataset description





