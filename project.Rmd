---
output:
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
title: ""
author: ""
date: ""
---

\thispagestyle{empty}


\vspace*{5cm}

\begin{center}
    {\LARGE \textbf{Predicting Boston Housing Prices}}\\[1em]
    {\large Akinyemi Apampao, xxxx}\\[1em]
    {\large David Fakolujo, xxxx}\\[1em]
    {\large Joshua Ogunbo, xxxx}\\[1em]
    {\large Prince Oloma, xxxx}\\[1em]
    {\large Ravin Jayasuriya, xxxx}\\
\end{center}

\newpage

```{r, echo = FALSE}
suppressPackageStartupMessages({
  library(mctest)
  library(ggplot2)
  library(GGally)
  library(car)
  library(olsrr)
  library(lmtest)
  library(knitr)
  library(MASS)
  })
```

# 1. INTRODUCTION

### 1.1.1 Context

This project falls within the domains of real estate analytics, urban economics, and urban planning. It focuses on understanding the factors that influence housing prices in Boston by analyzing a dataset derived from a census survey conducted in the 1970s. The dataset includes 13 features that may impact the value of homes in different neighborhoods. Our goal is to build regression models to predict the median value of owner-occupied homes (measured in thousands of dollars) and to identify which features have the most significant effect on housing prices.

### 1.1.2 Problem

The problem we aim to address is the difficulty in understanding which features most influence housing prices in Boston. Without clear insights into what drives property values, it becomes challenging for prospective buyers, analysts, or stakeholders to make informed decisions. Our objective is to develop a predictive model that estimates housing prices and helps identify the key factors contributing to those predictions.


### 1.1.3 Challenges

// ADD Data normality issue

\
\


# 1.2 OBJECTIVES

### 1.2.1 Overview

The housing market has been unpredictable in recent years, with prices rising in many areas and making it harder for people, especially young or first-time buyers, to afford a home. In this project, we’re working with housing data from Boston to build a model that can predict house prices. By exploring which features of a home are most strongly linked to its value, we hope to better understand what drives housing prices and help future buyers know what to look for.


### 1.2.2 Goals & Research Questions

The primary goal of this project is to build a predictive model that accurately estimates housing prices in Boston based on various property features. In doing so, we aim to uncover which features most strongly influence a home’s value.

To guide this objective, we explore the following research questions:

- Can we develop a reliable model to predict the median value of homes in Boston?  
- Which features are the most important in influencing housing prices?

\
\
\

# 2. METHODOLOGY

### 2.1 Data
The dataset used in this project consists of housing data collected from the Boston Standard Metropolitan Statistical Area (SMSA) in the 1970s. It contains 506 entries and includes 11 qualitative independent variables, 2 quantitative independent variables, and 1 quantitative dependent variable.

The dataset was originally collected as part of a census report and is considered open data. It is publicly available at:  
[https://lib.stat.cmu.edu/datasets/boston](https://lib.stat.cmu.edu/datasets/boston)

Below is a brief description of each variable:

- **CRIM**: Per capita crime rate by town. Indicates the level of crime in the area.
- **ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft. Reflects residential density.
- **INDUS**: Proportion of non-retail business acres per town. Indicates commercial land usage.
- **CHAS**: Charles River dummy variable (1 if tract bounds river; 0 otherwise). Indicates proximity to the Charles River.
- **NOX**: Nitric oxides concentration (parts per 10 million). Represents industrial pollution.
- **RM**: Average number of rooms per dwelling. Suggests spaciousness.
- **AGE**: Proportion of owner-occupied units built prior to 1940. Reflects the age of buildings in the area.
- **DIS**: Weighted distances to five Boston employment centres. Measures accessibility to work locations.
- **RAD**: Index of accessibility to radial highways. Higher values indicate better road access.
- **TAX**: Full-value property-tax rate per $10,000. Indicates the annual property tax burden.
- **PTRATIO**: Pupil-teacher ratio by town. Lower values suggest better educational facilities.
- **B**: 1000(Bk - 0.63)², where Bk is the proportion of Black residents by town.
- **LSTAT**: Percentage of the population considered lower status.
- **MEDV**: Median value of owner-occupied homes in $1000s. This is the dependent variable we aim to predict.[1]

### 2.2 Approach

In this project, we use a **predictive modeling approach** based on **multiple linear regression** to estimate the median value of homes in Boston. This method is well-suited for our goal of understanding how different housing features influence prices while also producing accurate predictions.

We believe multiple linear regression is an effective choice for this project for several reasons:

- **Interpretability**: The model provides clear and meaningful insights into how each variable affects housing prices, which is valuable for both analysis and decision-making.

- **No Multicollinearity**: VIF results confirmed the absence of multicollinearity, supporting the reliability and stability of the regression coefficients.

- **Well-Structured Data**: The dataset consists of numeric features that align well with the assumptions of linear regression, making it a natural modeling choice.

- **Proven Technique**: Linear regression is a widely accepted method in real estate analytics, with a long history of successful application in similar predictive tasks.

- **Strong Baseline**: This approach serves as a solid baseline for future comparisons with more complex models if needed, offering a balance of performance and simplicity.

### 2.3 Workflows

Below are the key steps:

 A. Test for Multicollinearity
 B. Create the Best Additive Model
 C. Create the Best Interaction Model
 D. Explore Higher Order Terms
 E. Test Multiple Regression Assumptions (linearity, independence, equal variance, normality, outliers)

### 2.4 Contributions
 
1. Akinyemi Apampa
 -Introduction writing: Context, Problem, Challenges
 -Initial data preprocessing and cleaning
 -Creating the additive regression model and interpreting results
 -Assisting with final report compilation and proofreading
 
 2. Ravin Jayasuriya
 -Multicollinearity testing and VIF analysis
 -Stepwise regression and reduced additive model selection
 -Writing and interpreting results of the additive model (t-tests, F-tests)
 -Assisting with final report compilation and proofreading
 
 3. Joshua Ogunbo
 -Developing and refining the full and reduced interaction models
 -Conducting and interpreting ANOVA tests for interaction models
 -Writing the section on interaction terms and model selection justification
 -Assisting with final report compilation and proofreading
 
 4. Prince Oloma Eworitsemoghan
 -Higher-order term exploration and model building (crim, zn, nox, rm, dis, rad, tax, lstat)
 -Identifying and incorporating significant polynomial terms
 -Writing the section on higher-order terms and their interpretation
 -Assisting with final report compilation and proofreading
 
 5. David Fakojulo
 -Addressing model assumption violations: log, Box-Cox, and WLS transformations
 -Finalizing the best regression model using weighted least squares
 -Performing and interpreting Shapiro-Wilk and Breusch-Pagan tests
 -Assisting with final report compilation and proofreading
 
 
# 3. MAIN RESULT OF THE ANALYSIS

**Data Import and Initial Inspection**

```{r echo = FALSE}
boston_data= read.csv("./BostonHousing.csv")
kable(head(boston_data, 3), caption = "First Three Rows of the Boston Housing Dataset")
```

**Recoding Categorical Variables for Model Compatibility**
```{r echo = FALSE}
boston_data$chas[which(boston_data$chas==0)] = "No"
boston_data$chas[which(boston_data$chas==1)] = "Yes"
kable(head(boston_data, 3), caption = "First Three Rows of the Boston Housing Dataset")
```


A. **Test for Multicollinearit**  
From the below, multicollinearity was not detected for any of the variables. 
```{r}
boston_additive <- lm(medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + b + lstat, data = boston_data)
```

```{r echo = FALSE}
invisible(capture.output(imcdiag(boston_additive, method = "VIF")))
```

```{r echo = FALSE}
vif_table <- data.frame(
  "Variable Name" = c("crim","zn","indus","chas","nox","rm","age","dis","rad","tax","ptratio","b","lstat"),
  "VIF" = c(1.7922,2.2928,3.9916,1.0740,4.3937,1.9337,3.1008,3.9559,7.4845,9.0086,1.7991,1.3485,2.9415),
  "Detection" = c(0,0,0,0,0,0,0,0,0,0,0,0,0)
)
kable(vif_table, caption = "TEST FOR MULTICOLINEARITY")
```

The **Variance Inflation Factor (VIF)** values for all predictors in the model are below the commonly used threshold of **10**, indicating that **multicollinearity is not a concern**.


B. **Create the Best Additive Model**  

We perform individual *t*-tests to assess the significance of each predictor in the model. The hypotheses are as follows:

- **Null Hypothesis**:  
  $$ H_0: \beta_1 = \beta_2 = \cdots = \beta_p = 0 $$

- **Alternative Hypothesis**:  
  $$ H_a: \text{At least one } \beta_i \ne 0, \quad \text{where } i = 1, 2, \ldots, p $$

These tests help determine whether each predictor contributes significantly to explaining the variability in the response variable.

```{r echo = FALSE}
boston_additive_summary <- summary(boston_additive)
boston_additive_coef_table <- as.data.frame(boston_additive_summary$coefficients)


knitr::kable(boston_additive_coef_table, digits = 4, caption = "Coefficient Estimates from the Additive Model")
invisible(capture.output(summary(boston_additive)))
```

From the regression output, we observe:

- **Significant predictors** (p-value < 0.05): `crim`, `zn`, `chas`, `nox`, `rm`, `dis`, `rad`, `tax`, `ptratio`, `b`, `lstat`
- **Insignificant predictors**: `indus` (p = 0.7383) and `age` (p = 0.9582)

Because the p-values of `indus` and `age` are greater than 0.05, we fail to reject the null hypotheses that their coefficients are zero. Thus, these variables do not significantly contribute to the prediction of `medv` in the presence of other variables.

The fitted multiple linear regression model is:
$$
\begin{aligned}
\hat{medv} =\ & 3.646 - 0.108 \cdot crim + 0.04642 \cdot zn \\
& + 0.02056 \cdot indus + 2.687 \cdot chas - 17.7666 \cdot nox \\
& + 3.8099 \cdot rm + 0.0007 \cdot age - 1.4756 \cdot dis \\
& + 0.3060 \cdot rad - 0.0123 \cdot tax - 0.9527 \cdot ptratio \\
& + 0.0093 \cdot b- 0.5248 \cdot lstat
\end{aligned}
$$
\

### Building the Reduced Additive model

We remove the variables `indus` and `age`, which were found to be statistically insignificant in the full model, and test the following hypotheses:

- **Null Hypothesis**:  
  $$ H_0: \beta_{\text{indus}} = \beta_{\text{age}} = 0 $$

- **Alternative Hypothesis**:  
  $$ H_a: \text{At least one of } \beta_{\text{indus}}, \beta_{\text{age}} \ne 0 $$
A high p-value would indicate that removing `indus` and `age` does not significantly worsen the model, and thus the reduced model is preferred for its simplicity.
  
```{r}
reduced_additive_model <- lm(
  formula = medv ~ crim + zn + factor(chas) + nox + rm +
    dis + rad + tax + ptratio + b + lstat,
  data = boston_data
)
```

```{r echo = FALSE}
reduced_model_summary <- summary(reduced_additive_model)
reduced_coef_table <- as.data.frame(reduced_model_summary$coefficients)


knitr::kable(reduced_coef_table, digits = 4, 
  caption = "Coefficient Estimates from the Reduced Additive Model")
invisible(capture.output(summary(reduced_additive_model)))
```

The reduced model shows that **all included variables have p-values less than 0.05**, indicating that they are statistically significant.

We will now run a **global F-test (ANOVA)** to assess whether removing the variables `indus` and `age` significantly worsens the model fit.


```{r echo = FALSE}
anova_results <- anova(reduced_additive_model, boston_additive)

knitr::kable(anova_results, digits = 4, 
  caption = "ANOVA Comparison of Reduced and Full Additive Model")
```

Since the p-value is **0.9443**, which is much greater than the threshold of **0.05**, we **fail to reject the null hypothesis**. This indicates that `indus` and `age` do not significantly improve the model. Hence, the reduced model is more appropriate as it maintains model quality while eliminating unnecessary predictors.


### Stepwise Model Selection**

To further validate the choice of predictors and identify the most parsimonious model, we used the `ols_step_both_p()` function from the **olsrr** package to perform stepwise selection based on p-values.

```{r echo = FALSE}
stepmod=ols_step_both_p(boston_additive,p_enter = 0.05, p_remove = 0.1, details=FALSE)

stepwise_model_summary <- summary(stepmod$model)
stepwise_coef_table <- as.data.frame(stepwise_model_summary$coefficients)


knitr::kable(
  stepwise_coef_table,
  digits = 4,
  caption = "Coefficient Estimates from the Stepwise-Selected Model"
)

invisible(capture.output(summary(stepmod$model)))
```

The selected model is the same as the reduced model obtained earlier. 
Hence, the **final additive model is**:

$$
\begin{aligned}
\hat{medv}_i =\ & 36.3411 - 0.5226 \cdot lstat_i + 3.8016 \cdot rm_i \\
& - 0.9465 \cdot ptratio_i - 1.4927 \cdot dis_i - 17.3760 \cdot nox_i + 2.7187 \cdot chas_i \\
& + 0.0093 \cdot b_i + 0.0458 \cdot zn_i - 0.1084 \cdot crim_i + 0.2996 \cdot rad_i - 0.0118 \cdot tax_i
\end{aligned}
$$

Where:

$$
\begin{aligned}
&\quad chas_i = 1 \quad \text{if the tract bounds the Charles River, and 0 otherwise}
\end{aligned}
$$
C. **Create the Best Interaction Model**

A full two-way interaction model was constructed by including all possible interaction terms among the predictors in the final additive model obtained from stepwise selection.

```{r echo = FALSE}
interaction_model <- lm(
  formula = medv ~ (crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat)^2,
  data = boston_data
)

interaction_model_summary <- summary(interaction_model)
interaction_coef_table <- as.data.frame(interaction_model_summary$coefficients)

knitr::kable(
  interaction_coef_table,
  digits = 4,
  caption = "Coefficient Estimates from the Full Two-Way Interaction Model"
)

invisible(capture.output(summary(interaction_model)))
```

A full two-way interaction model was constructed by including all possible interaction terms among the predictors in the final additive model obtained from stepwise selection.

The full interaction model includes several interaction terms with **p-values greater than 0.05**, indicating that they are not statistically significant. These insignificant interactions were dropped to create a **reduced interaction model**.


```{r echo = FALSE}
reduced_interaction_model <- lm(
  formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat +
    crim:zn + crim:chas + crim:nox + crim:rm + crim:dis + crim:rad + crim:tax + crim:b + crim:lstat +
    zn:dis + zn:tax +
    chas:nox + chas:rm + chas:lstat +
    nox:rad + nox:tax +
    rm:dis + rm:ptratio + rm:b + rm:lstat +
    dis:rad + dis:lstat +
    b:lstat,
  data = boston_data
)


reduced_interaction_summary <- summary(reduced_interaction_model)
reduced_interaction_coef_table <- as.data.frame(reduced_interaction_summary$coefficients)

knitr::kable(
  reduced_interaction_coef_table,
  digits = 4,
  caption = "Coefficient Estimates from the Reduced Interaction Model"
)

invisible(capture.output(summary(reduced_interaction_model)))
```

The output of the reduced interactive model showed more insignificant interactions, which were further dropped.

```{r echo = FALSE}
reduced_interaction_model2 <- lm(
  formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat +
    crim:zn + crim:chas + crim:rad + crim:tax +
    zn:dis +
    chas:nox + chas:rm +
    nox:rad + nox:tax +
    rm:dis + rm:ptratio + rm:lstat +
    dis:rad + dis:lstat +
    b:lstat,
  data = boston_data
)

reduced_interaction_model2_summary <- summary(reduced_interaction_model2)
reduced_interaction_model2_coef_table <- as.data.frame(reduced_interaction_model2_summary$coefficients)

knitr::kable(
  reduced_interaction_model2_coef_table,
  digits = 4,
  caption = "Table: Coefficient Estimates from the Final Reduced Interaction Model"
)

invisible(capture.output(summary(reduced_interaction_model2)))
```

The further reduced model then showed nox:tax as insignificant, which was also dropped.

```{r echo = FALSE}
reduced_interaction_model3 <- lm(
  formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat +
    crim:zn + crim:chas + crim:rad + crim:tax +
    zn:dis +
    chas:nox + chas:rm +
    nox:rad +
    rm:dis + rm:ptratio + rm:lstat +
    dis:rad + dis:lstat +
    b:lstat,
  data = boston_data
)

reduced_interaction_model3_summary <- summary(reduced_interaction_model3)
reduced_interaction_model3_coef_table <- as.data.frame(reduced_interaction_model3_summary$coefficients)

knitr::kable(
  reduced_interaction_model3_coef_table,
  digits = 4,
  caption = "Table: Coefficient Estimates from the Third Reduced Interaction Model"
)

invisible(capture.output(summary(reduced_interaction_model3)))
```

Now that all interactions were significant, an f-test was run to compare reduced interactive model and full interaction model

```{r echo = FALSE}
all_reduced_interaction_full_anova <- data.frame(anova(reduced_interaction_model3, interaction_model))

knitr::kable(
  all_reduced_interaction_full_anova,
  digits = 9,
  caption = "ANOVA Comparison of Third Reduced Interaction Model and Full Interaction Model"
)
```

Based on the summary above, the p-value is $8.242e-12<0.05$, suggesting the null hypothesis should be rejected. Also, the adjusted $R^2_{adj}$ and RSE of the full interaction model are 0.888 and 3.078 respectively, while the adjusted $R^2_{adj}$ and RSE of the reduced interaction model are 0.8627 and 3.408 respectively. 

These suggest the full interaction model should be preferred. However, the full interaction model has a number of insignificant interactions, while the reduced interaction model has only significant interactions. Even though the anova test, adjusted $R^2_{adj}$ and RSE suggest preferring the full interaction model, there isn't a major difference between the adjusted $R^2_{adj}$ and RSE of the two models.

We would choose the reduced model because it retains all significant interactions while eliminating insignificant ones, ensuring better interpretability and avoiding unnecessary complexity without a substantial loss in explanatory power.



D. **Explore Higher Order Terms**      

To check for possible higher order relationships, we explored all pairwise combinations of continuous variables in scatterplots to see how the response variable looked with respect to each of the continuous additive predictors

```{r}
higher_order_data = data.frame(boston_data$medv, boston_data$crim, boston_data$zn, boston_data$chas, boston_data$nox, boston_data$rm, boston_data$dis, boston_data$rad, boston_data$tax, boston_data$ptratio, boston_data$b, boston_data$lstat)

ggpairs(higher_order_data,lower = list(continuous = wrap("smooth_loess", color = "red"), combo = "facethist", discrete = "facetbar", na = "na"))
```

It looks like the variables that might be worth exploring for possible higher order relationships with medv are crim, zn, nox, rm, dis, rad, tax, lstat. Each of these variables were tested as follows:

# Possible higher order relationship exploration for crim

```{r}
higher_order_crim_2 = lm(formula = medv ~ crim + I(crim^2) + zn + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_crim_2)
```

```{r}
higher_order_crim_3 = lm(formula = medv ~ crim + I(crim^2) + I(crim^3) + zn + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_crim_3)
```

# Possible higher order relationship exploration for zn
```{r}
higher_order_zn_2 = lm(formula = medv ~ crim + zn + I(zn^2) + chas + nox + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_zn_2)
```

# Possible higher order relationship exploration for nox
```{r}
higher_order_nox_2 = lm(formula = medv ~ crim + zn + chas + nox + I(nox^2) + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_nox_2)
```

```{r}
higher_order_nox_3 = lm(formula = medv ~ crim + zn + chas + nox + I(nox^2) + I(nox^3) + rm+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_nox_3)
```

# Possible higher order relationship exploration for rm
```{r}
higher_order_rm_2 = lm(formula = medv ~ crim + zn + chas + nox + rm+ I(rm^2)+ dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_rm_2)
```

```{r}
higher_order_rm_3 = lm(formula = medv ~ crim + zn + chas + nox + rm+ I(rm^2)+ I(rm^3) + dis + rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_rm_3)
```

# Possible higher order relationship exploration for dis
```{r}
higher_order_dis_2 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + I(dis^2)+ rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_dis_2)
```


```{r}
higher_order_dis_3 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + I(dis^2)+ I(dis^3)+ rad + tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_dis_3)
```


# Possible higher order relationship exploration for rad
```{r}
higher_order_rad_2 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + I(rad^2)+ tax + ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_rad_2)
```

# Possible higher order relationship exploration for tax
```{r}
higher_order_tax_2 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + I(tax^2)+ ptratio + b + lstat + crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_tax_2)
```

# Possible higher order relationship exploration for lstat
```{r}
higher_order_lstat_2 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_2)
```

```{r}
higher_order_lstat_3 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_3)
```

```{r}
higher_order_lstat_4 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_4)
```

```{r}
higher_order_lstat_5 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_5)
```

```{r}
higher_order_lstat_6 = lm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5)+ I(lstat^6)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_lstat_6)
```

Based on the exploration above, the significant higher order relationships were then added to the reduced interactive model

```{r}
higher_order_interaction_model = lm(formula = medv ~ crim + I(crim^2)+ zn + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5)+ crim:zn + crim:chas  +crim:rad+ crim:tax + zn:dis+ chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ rm:lstat+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_interaction_model)
```

The new insignificant variables were then dropped to get a reduced higher order interaction model.

```{r}
higher_order_interaction_model_2 = lm(formula = medv ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_interaction_model_2)
```
The higher order interaction model with significant variables is:

$\hat{medv} = 36.341145 - 0.108413crim_{i} +0.045845zn_{i} +2.718716chas_{i} -17.376023nox_{i} + 3.801579rm_{i} -1.492711dis_{i} + 0.299608rad_{i} -0.011778tax_{i} -0.946525ptratio_{i} +0.009291b_{i} - 0.522553lstat_{i}$

where $chas_{i}$ is 1 if the tract bounds Charles River and 0 if otherwise

E. **Testing Multiple Regression Assumptions**  
The following multiple regression assumptions were tested to check if the model is trustworthy

# Linearity Assumption

A scatter plot of the distribution of residuals (errors) vs fitted values (predicted values) was plotted below

```{r}
ggplot(higher_order_interaction_model_2, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)+
ggtitle("Residual plot: Residual vs Fitted values")
```

There appears to be no pattern of the residuals at all, indicating that the model passes the linearity assumption that there is a straight-line (linear) relationship between the predictors and the response.

## Independence Assumption

In the Boston housing dataset, the subjects were not related to time, space, or group, so we can be pretty sure that their measurements are independent.


## Equal Variance Assumption

The residuals plot in the linearity assumption section indicates a smooth fit to the residuals, which is good. In addition to the residuals plot, a scale-location plot between fitted values and standardized residuals was also plotted to show if the residuals are spread equally along the ranges of predictors

```{r}
ggplot(higher_order_interaction_model_2, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth( colour = "green4")+
ggtitle("Scale-Location plot : Standardized Residual vs Fitted values")
```

Based on the plot, we can see that the scale-location plot is quite horizontal, and there is not any funneling in the residual plot, indicating equal variance.

The Breusch-Pagan test was then run as a more formal way to assess if we have homo/heteroscedasticity using the following hypotheses:

$$
\begin{aligned}
H_0:&\mbox{ heteroscedasticity is not present (homoscedasticity)}\\
H_a~:&\mbox{ heteroscedasticity is present} \\
\end{aligned}
$$

```{r}
bptest(higher_order_interaction_model_2)
```

The p-value of the Breusch-Pagan test is less than 0.05 (2.505e-05), so we fail to reject the null hypothesis, indicating we do have heteroscedasticity. 

An attempt to address the heteroscedasticity, in addition to any other assumption failure will be done after testing all other assumptions.



## NORMALITY

The multiple linear regression analysis requires that the errors between observed and predicted values (i.e., the residuals of the regression) should be normally distributed.

A histogram and q-q plot were developed to check if the residuals were normally distributed or not.


```{r}
ggplot(data=boston_data, aes(residuals(higher_order_interaction_model_2))) +
geom_histogram(breaks = seq(-1,1,by=0.1), col="green3", fill="green4") +
labs(title="Histogram for residuals") +
labs(x="residuals", y="Count")
```


```{r}
ggplot(boston_data, aes(sample=higher_order_interaction_model_2$residuals)) +
stat_qq() + stat_qq_line()
```

Based on the plots above, it appears the residuals are not normal. 

A Shapiro-Wilk normality statistical test was also conducted to confirm that the residuals are not normally distributed

Hypotheses
$$
\begin{aligned}
H_0:&\mbox{ the sample data are significantly normally distributed}\\
H_a:&\mbox{ the sample data are not significantly normally distributed } \\
\end{aligned}
$$

```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_2))
```

The p-value of the Shapiro-Wilk normality test is less than 0.05 (4.088e-13), so we fail to reject the null hypothesis, indicating we do not have normality. 

An attempt to address the normality will be done after testing all other assumptions.


## OUTLIERS
The below approaches were explored to find and evaluate outliers or influential points.

1. Residuals vs Leverage Plot

```{r}
plot(higher_order_interaction_model_2, 5)
```

The plot above shows that all cases are well inside of the Cook’s distance lines, indicating no outliers or no influential points.

2. Cook’s Distance

```{r}
plot(higher_order_interaction_model_2,pch=18,col="red",which=c(4))
```

Based on the consensus that a value of more than 1 indicates an influential value, the cook's distance plot above indicates there are no influential values.


3. Leverage points

```{r}
lev=hatvalues(higher_order_interaction_model_2)
p = length(coef(higher_order_interaction_model_2))
n = nrow(boston_data)
outlier2p = lev[lev>(2*p/n)]
outlier3p = lev[lev>(3*p/n)]
print("h_I>2p/n, outliers are")

print(outlier2p)
```

```{r}
print("h_I>3p/n, outliers are")

print(outlier3p)
```

```{r}
plot(rownames(boston_data),lev, main = "Leverage in Boston Housing Dataset", xlab="observation", ylab = "Leverage Value")
abline(h = 2 *p/n, lty = 1)
abline(h = 3 *p/n, lty = 1)
```

Based on the above plot, it appears we have high leverage points but none of them appear to be particularly influential (no points with a concerning cooks distance). Hence, it appears we do not have any outliers that could pose problems.




## Dealing with Heteroscedasticity and Normality
Now that all multiple regression assumptions have been tested to check if the model is trustworthy, we then made attempts to address the heteroscedasticity and lack of normality. 

We started by making a log transformation of the predictor so the difference between big and small numbers relatively becomes small.

```{r}
higher_order_interaction_model_log = lm(formula = log(medv) ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_interaction_model_log)
```
Some insignificant variables were observed after the log transformation, which were subsequently dropped from the model

```{r}
higher_order_interaction_model_log_2 = lm(formula = log(medv) ~ crim + I(crim^2) + chas + nox+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio + dis:lstat, data = boston_data)
summary(higher_order_interaction_model_log_2)
```

The log-transformed model was tested for homoscedasticity and normality. 

```{r}
#Testing for Homoscedasticity
bptest(higher_order_interaction_model_log_2)
```

The p-value of the Breusch-Pagan test is less than 0.05 (4.085e-07), indicating we still have heteroscedasticity. 

```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_log_2))
```

The p-value of the Shapiro-Wilk normality test is less than 0.05 (9.614e-12), so we fail to reject the null hypothesis, indicating we do not have normality. 

Since the log-transformation approach did not resolve the heteroscedasticity or lack of normality, we then explored the Box-Cox transformations approach by identifying $\hat{\lambda}$, the maximum likelihood estimate of $\lambda$ to use in the power transformation

```{r}
bc=boxcox(higher_order_interaction_model_2,lambda=seq(-1,1))
```


```{r}
#extract best lambda
bestlambda=bc$x[which(bc$y==max(bc$y))]
bestlambda
```

The best lambda estimate was then used to transform the model as follows:

```{r}
higher_order_interaction_model_2_box = lm(formula = (((medv^0.4949495)-1)/0.4949495) ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ dis:rad + dis:lstat+ b:lstat, data = boston_data)
summary(higher_order_interaction_model_2_box)
```

The best lambda transformed model was then tested for homoscedasticity and normality. 

```{r}
bptest(higher_order_interaction_model_2_box)
```
The p-value of the Breusch-Pagan test is less than 0.05 (5.277e-05), indicating we still have heteroscedasticity. 

```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_2_box))
```

The p-value of the Shapiro-Wilk normality test is less than 0.05 (5.297e-10), so we fail to reject the null hypothesis, indicating we do not have normality.


Since the box-cox transformation approach did not resolve the heteroscedasticity or lack of normality also, we then explored the weighted least squares regression method, which is an application of the more general concept of generalized least squares. This method gives less weight to observations with high variance as follows:


```{r}
#Determine absolute residuals
resid = resid(higher_order_interaction_model_2)
```

```{r}
#Fit a model to the residuals to model the variance pattern
resid_model = lm(formula = log(resid^2) ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5), data = boston_data)
```

```{r}
#Create weights using fitted variances
fitted_var = exp(fitted(resid_model))
weights = 1 / fitted_var
```


```{r}
#Re-fit the model with weights
higher_order_interaction_model_wls = lm(formula = medv ~ crim + I(crim^2) + chas + nox + I(nox^2)+ rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio+ dis:rad + dis:lstat+ b:lstat, data = boston_data, weights = weights)
summary(higher_order_interaction_model_wls)
```

Some insignificant variables were observed, which were subsequently dropped from the model

```{r}
higher_order_interaction_model_wls_2 = lm(formula = medv ~ crim + I(crim^2) + chas + nox + rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + nox:rad + rm:dis + rm:ptratio + dis:lstat+ b:lstat, data = boston_data, weights = weights)
summary(higher_order_interaction_model_wls_2)
```
A further insignificant variables was observed, which was also dropped from the model

```{r}
higher_order_interaction_model_wls_3 = lm(formula = medv ~ crim + I(crim^2) + chas + nox + rm+ I(rm^2)+ dis + I(dis^2)+ rad + tax + ptratio + b + lstat + I(lstat^2)+ I(lstat^3)+ I(lstat^4)+ I(lstat^5) + crim:chas  +crim:rad+ crim:tax + chas:nox +chas:rm + rm:dis + rm:ptratio + dis:lstat+ b:lstat, data = boston_data, weights = weights)
summary(higher_order_interaction_model_wls_3)
```
The model derived from the weighted least squares regression method was tested for homoscedasticity and normality.

```{r}
bptest(higher_order_interaction_model_wls_3)
```

The p-value of the Breusch-Pagan test is greater than 0.05 (0.9934), indicating we now finally have homoscedasticity. 

```{r}
#Testing for Normality
shapiro.test(residuals(higher_order_interaction_model_wls_3))
```

The p-value of the Shapiro-Wilk normality test is less than 0.05 (9.614e-12), indicating we still do not have normality.




The linearity and outliers assumptions would be tested again to confirm they are still met

# Linearity Assumption

```{r}
ggplot(higher_order_interaction_model_wls_3, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4")+
ggtitle("Residual plot: Residual vs Fitted values")
```

Based on the updated model, there appears to be no pattern of the residuals at all, indicating that the model still passes the linearity assumption that there is a straight-line (linear) relationship between the predictors and the response.

```{r}
ggplot(higher_order_interaction_model_wls_3, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)
```


## Outliers

1. Residuals vs Leverage Plot

```{r}
plot(higher_order_interaction_model_wls_3, 5)
```

The plot above shows that all cases are well inside of the Cook’s distance lines, indicating no outliers or no influential points.

2. Cook’s Distance

```{r}
plot(higher_order_interaction_model_wls_3,pch=18,col="red",which=c(4))
```

Based on the consensus that a value of more than 1 indicates an influential value, the cook's distance plot above indicates there are still no influential values.


3. Leverage points

```{r}
lev=hatvalues(higher_order_interaction_model_2)
p = length(coef(higher_order_interaction_model_2))
n = nrow(boston_data)
outlier2p = lev[lev>(2*p/n)]
outlier3p = lev[lev>(3*p/n)]
print("h_I>2p/n, outliers are")

print(outlier2p)
```

```{r}
print("h_I>3p/n, outliers are")

print(outlier3p)
```

```{r}
plot(rownames(boston_data),lev, main = "Leverage in Boston Housing Dataset", xlab="observation", ylab = "Leverage Value")
abline(h = 2 *p/n, lty = 1)
abline(h = 3 *p/n, lty = 1)
```

Based on the above plot, it appears we still have high leverage points but none of them appear to be particularly influential (no points with a concerning cooks distance). Hence, it appears we do not have any outliers that could pose problems.


# 4. CONCLUSION AND DISCUSSION

We developed a predictive model using multiple linear regression to estimate the median value of owner-occupied homes in Boston based on 13 housing and neighborhood features. After loading the data, we tested for multicollinearity; where none was detected. Next, we built a baseline additive model and refined it by removing insignificant predictors. Afterwards, Interaction effects were explored, and a reduced interaction model was selected based on significance and model performance. Then we explored higher order terms to capture non-linear relationships, and we iteratively refined the model to retain only the significant predictors. Finally we tested the multiple regression assumptions: which included linearity, independence, equal variance, normality, and outliers. While the final model met most assumptions, the residuals remained non-normal despite attempts with log, Box-Cox, and weighted least squares transformations. Ultimately, the weighted least squares regression model was selected as the final model, as it effectively treated heteroscedasticity, improved model interpretability, and maintained linearity and robustness against outliers. This comprehensive modeling process provided insights into key factors influencing housing prices, our final predictive model is: 

$\hat{medv} = 72.03-6.281crim+0.003005I(crim^2)+24.61chas-15.37nox-12.46rm+1.827I(rm^2)-7.739dis+0.1715I(dis^2)+0.3466rad-0.01298tax+2.704ptratio+0.01990b-4.951lstat+0.5050I(lstat^2)-0.02711I(lstat^3)+0.0006835I(lstat^4)-0.000006415I(lstat^5)+1.485crim:chas-0.3624crim:rad+0.02194crim:tax-22.75chas:nox-1.872chas:rm+0.6587rm:dis-0.5220rm:ptratio+0.09832dis:lstat-0.0006878b:lstat$


Based off of this, the key factors that predict the median value (in thousands of dollars) are: 

crim, I(crim^2), chas, nox, rm, I(rm^2), dis, I(dis^2), rad, tax, ptratio, b, lstat, I(lstat^2), I(lstat^3), I(lstat^4), I(lstat^5), crim:chas, crim:rad, crim:tax, chas:nox, chas:rm, rm:dis, rm:ptratio, dis:lstat, b:lstat



4.1 Approach
The approach we took does seem logically sound. Starting with a basic multiple linear regression model allowed us to build a strong baseline and interpret the effects of various predictors on housing prices. Looking at interaction terms and higher-order relationships enabled us to improve model complexity effectively. Additionally, testing each of the regression assumptions step-by-step ensured that our final model would be valid statistically. Some issues we had were that there was consistent violation of the normality assumption, even with various transformations. Possibilities for improvement are possibly running non-linear or machine learning models. They may be able to handle normality and allow us to create a better predictive model. 


4.2 Future Work










# 5. REFERENCES
[1] - Dataset description





